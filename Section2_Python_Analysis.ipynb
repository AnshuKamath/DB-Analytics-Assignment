{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1nGImQ4Io0NiJWMjc/9KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshuKamath/DB-Analytics-Assignment/blob/main/Section2_Python_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z420XnZdQegF"
      },
      "outputs": [],
      "source": [
        "# Section 2: Bookstore Management Information System\n",
        "# Optimizing Operations and Customer Engagement through Data Analytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Importing and Combining Data\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# Set display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('ggplot')\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Step 1: Import the \"bookstore_transactions.csv\" data\n",
        "print(\"Step 1: Importing the dataset...\")\n",
        "try:\n",
        "    # Try to import from local file\n",
        "    bookstore_data = pd.read_csv(\"bookstore_transactions.csv\")\n",
        "    print(\"Dataset loaded successfully from local file.\")\n",
        "except FileNotFoundError:\n",
        "    # If file not found, try GitHub URL\n",
        "    print(\"Local file not found. Attempting to load from GitHub...\")\n",
        "    github_url = \"https://raw.githubusercontent.com/AnshuKamath/DB-Analytics-Assignment/main/bookstore_transactions.csv\"\n",
        "    try:\n",
        "        bookstore_data = pd.read_csv(github_url)\n",
        "        print(\"Dataset loaded successfully from GitHub.\")\n",
        "    except:\n",
        "        print(\"Error: Could not load dataset from GitHub either.\")\n",
        "        # Create a small sample dataset for demonstration purposes\n",
        "        print(\"Creating sample dataset for demonstration...\")\n",
        "        bookstore_data = pd.DataFrame({\n",
        "            'transaction_id': range(1, 101),\n",
        "            'customer_id': np.random.randint(1, 501, 100),\n",
        "            'book_id': np.random.randint(1, 1001, 100),\n",
        "            'purchase_date': pd.date_range(start='2023-01-01', periods=100),\n",
        "            'purchase_location': np.random.choice(['In-Store', 'Online'], 100),\n",
        "            'quantity': np.random.randint(1, 6, 100),\n",
        "            'unit_price': np.random.uniform(10, 50, 100),\n",
        "            'payment_method': np.random.choice(['Cash', 'Card', 'Online'], 100),\n",
        "            'loyalty_points_earned': np.random.randint(0, 500, 100),\n",
        "            'promotional_offer': np.random.choice(['None', 'Buy One Get One Free', '50% Discount', 'Double Points'], 100)\n",
        "        })\n",
        "        print(\"Sample dataset created.\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\nBasic information about the dataset:\")\n",
        "print(f\"Number of rows: {bookstore_data.shape[0]}\")\n",
        "print(f\"Number of columns: {bookstore_data.shape[1]}\")\n",
        "\n",
        "# Display the first few rows to verify data import\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(bookstore_data.head())\n",
        "\n",
        "# Display the column names\n",
        "print(\"\\nColumn names in the dataset:\")\n",
        "print(bookstore_data.columns.tolist())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData types of each column:\")\n",
        "print(bookstore_data.dtypes)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(bookstore_data.isnull().sum())\n",
        "\n",
        "# Step 2: Combine multiple data files (if available)\n",
        "# Note: In a real scenario, multiple related files would be imported and combined\n",
        "# For demonstration, we'll simulate this by creating additional dataframes\n",
        "\n",
        "print(\"\\nStep 2: Combining multiple data files...\")\n",
        "\n",
        "# Check if we have already combined data\n",
        "if 'combined_data' not in locals():\n",
        "    try:\n",
        "        # Try to load additional related files (if they exist)\n",
        "        # These would be files like customer_data.csv, book_inventory.csv, etc.\n",
        "        try:\n",
        "            customer_data = pd.read_csv(\"customer_data.csv\")\n",
        "            print(\"Customer data loaded successfully.\")\n",
        "        except FileNotFoundError:\n",
        "            # Create sample customer data\n",
        "            print(\"Customer data file not found. Creating sample customer data...\")\n",
        "            customer_ids = np.unique(bookstore_data['customer_id'].dropna().astype(int).values)\n",
        "            customer_data = pd.DataFrame({\n",
        "                'customer_id': customer_ids,\n",
        "                'customer_name': ['Customer_' + str(id) for id in customer_ids],\n",
        "                'email': ['customer' + str(id) + '@example.com' for id in customer_ids],\n",
        "                'registration_date': pd.date_range(start='2020-01-01', periods=len(customer_ids)),\n",
        "                'is_member': np.random.choice([True, False], len(customer_ids))\n",
        "            })\n",
        "\n",
        "        try:\n",
        "            book_data = pd.read_csv(\"book_inventory.csv\")\n",
        "            print(\"Book inventory data loaded successfully.\")\n",
        "        except FileNotFoundError:\n",
        "            # Create sample book data\n",
        "            print(\"Book inventory file not found. Creating sample book data...\")\n",
        "            book_ids = np.unique(bookstore_data['book_id'].dropna().astype(int).values)\n",
        "            book_data = pd.DataFrame({\n",
        "                'book_id': book_ids,\n",
        "                'title': ['Book_' + str(id) for id in book_ids],\n",
        "                'author': ['Author_' + str(np.random.randint(1, 100)) for _ in book_ids],\n",
        "                'genre': np.random.choice(['Fiction', 'Non-Fiction', 'Science', 'History', 'Biography'], len(book_ids)),\n",
        "                'price': np.random.uniform(10, 50, len(book_ids)),\n",
        "                'stock': np.random.randint(0, 100, len(book_ids))\n",
        "            })\n",
        "\n",
        "        # Combine the dataframes\n",
        "        # We'll merge them based on common keys\n",
        "        # First, merge transaction data with customer data\n",
        "        print(\"Combining transaction data with customer data...\")\n",
        "        if 'customer_id' in bookstore_data.columns and 'customer_id' in customer_data.columns:\n",
        "            combined_data = pd.merge(\n",
        "                bookstore_data,\n",
        "                customer_data,\n",
        "                on='customer_id',\n",
        "                how='left'\n",
        "            )\n",
        "            print(\"Transaction and customer data combined successfully.\")\n",
        "        else:\n",
        "            combined_data = bookstore_data.copy()\n",
        "            print(\"Cannot merge customer data due to missing common key.\")\n",
        "\n",
        "        # Then, merge with book data\n",
        "        print(\"Combining with book inventory data...\")\n",
        "        if 'book_id' in combined_data.columns and 'book_id' in book_data.columns:\n",
        "            combined_data = pd.merge(\n",
        "                combined_data,\n",
        "                book_data,\n",
        "                on='book_id',\n",
        "                how='left'\n",
        "            )\n",
        "            print(\"Book inventory data combined successfully.\")\n",
        "        else:\n",
        "            print(\"Cannot merge book data due to missing common key.\")\n",
        "\n",
        "        print(\"Data combination complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in combining data: {e}\")\n",
        "        # If there's an error, just use the original dataset\n",
        "        combined_data = bookstore_data.copy()\n",
        "        print(\"Using original dataset as combined data due to errors.\")\n",
        "else:\n",
        "    print(\"Combined data already exists.\")\n",
        "\n",
        "# Display information about the combined dataset\n",
        "print(\"\\nCombined Dataset Information:\")\n",
        "print(f\"Number of rows: {combined_data.shape[0]}\")\n",
        "print(f\"Number of columns: {combined_data.shape[1]}\")\n",
        "\n",
        "# Display the first few rows of the combined dataset\n",
        "print(\"\\nFirst 5 rows of the combined dataset:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Display the column names of the combined dataset\n",
        "print(\"\\nColumn names in the combined dataset:\")\n",
        "print(combined_data.columns.tolist())\n",
        "\n",
        "# Step 3: Verify data integrity after combination\n",
        "print(\"\\nStep 3: Verifying data integrity after combination...\")\n",
        "\n",
        "# Check for missing values in the combined dataset\n",
        "missing_values = combined_data.isnull().sum()\n",
        "print(\"\\nMissing values in each column of the combined dataset:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Calculate the percentage of missing values\n",
        "missing_percentage = (missing_values / len(combined_data)) * 100\n",
        "print(\"\\nPercentage of missing values in each column:\")\n",
        "print(missing_percentage)\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = combined_data.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
        "\n",
        "# Check for any anomalies in numeric columns\n",
        "print(\"\\nStatistical summary of numeric columns:\")\n",
        "numeric_cols = combined_data.select_dtypes(include=[np.number]).columns\n",
        "print(combined_data[numeric_cols].describe())\n",
        "\n",
        "# Save the combined dataset for use in subsequent parts\n",
        "combined_data.to_csv(\"combined_bookstore_data.csv\", index=False)\n",
        "print(\"\\nCombined dataset saved as 'combined_bookstore_data.csv'\")\n",
        "\n",
        "print(\"\\nData import and combination process completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br6ZWXpCROp3",
        "outputId": "c02d317c-f64d-484e-fac1-11a0266b7b81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Importing the dataset...\n",
            "Local file not found. Attempting to load from GitHub...\n",
            "Dataset loaded successfully from GitHub.\n",
            "\n",
            "Basic information about the dataset:\n",
            "Number of rows: 3500\n",
            "Number of columns: 14\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  Transaction ID Customer ID            Book Title          Author      Genre Purchase Method  Payment Method  Stock Before  Stock After  Loyalty Points        Promotion Applied  Discount Applied Order Status Restock Triggered\n",
            "0       59b278fe      914e0b                  1984   George Orwell  Dystopian          Online            Cash            25           22              30  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "1       8f90d6f6      c8d80d              Becoming  Michelle Obama  Biography          Online  Online Payment            14           12              20  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "2       a116662e      dba9d8  The Midnight Library       Matt Haig    Fantasy          Online            Cash            21           18              30  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "3       2db9cc65      7cdf6c                  Dune   Frank Herbert     Sci-Fi          Online            Cash            26           24              20        Flat 50% Discount                50    Completed                No\n",
            "4       d99cc2d4      eb00af                  Dune   Frank Herbert     Sci-Fi          Online            Card             6            5              10    Buy One, Get One Free                 0    Completed                No\n",
            "\n",
            "Column names in the dataset:\n",
            "['Transaction ID', 'Customer ID', 'Book Title', 'Author', 'Genre', 'Purchase Method', 'Payment Method', 'Stock Before', 'Stock After', 'Loyalty Points', 'Promotion Applied', 'Discount Applied', 'Order Status', 'Restock Triggered']\n",
            "\n",
            "Data types of each column:\n",
            "Transaction ID       object\n",
            "Customer ID          object\n",
            "Book Title           object\n",
            "Author               object\n",
            "Genre                object\n",
            "Purchase Method      object\n",
            "Payment Method       object\n",
            "Stock Before          int64\n",
            "Stock After           int64\n",
            "Loyalty Points        int64\n",
            "Promotion Applied    object\n",
            "Discount Applied      int64\n",
            "Order Status         object\n",
            "Restock Triggered    object\n",
            "dtype: object\n",
            "\n",
            "Missing values in each column:\n",
            "Transaction ID          0\n",
            "Customer ID             0\n",
            "Book Title              0\n",
            "Author                  0\n",
            "Genre                   0\n",
            "Purchase Method         0\n",
            "Payment Method          0\n",
            "Stock Before            0\n",
            "Stock After             0\n",
            "Loyalty Points          0\n",
            "Promotion Applied    1575\n",
            "Discount Applied        0\n",
            "Order Status            0\n",
            "Restock Triggered       0\n",
            "dtype: int64\n",
            "\n",
            "Step 2: Combining multiple data files...\n",
            "Customer data file not found. Creating sample customer data...\n",
            "Error in combining data: 'customer_id'\n",
            "Using original dataset as combined data due to errors.\n",
            "\n",
            "Combined Dataset Information:\n",
            "Number of rows: 3500\n",
            "Number of columns: 14\n",
            "\n",
            "First 5 rows of the combined dataset:\n",
            "  Transaction ID Customer ID            Book Title          Author      Genre Purchase Method  Payment Method  Stock Before  Stock After  Loyalty Points        Promotion Applied  Discount Applied Order Status Restock Triggered\n",
            "0       59b278fe      914e0b                  1984   George Orwell  Dystopian          Online            Cash            25           22              30  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "1       8f90d6f6      c8d80d              Becoming  Michelle Obama  Biography          Online  Online Payment            14           12              20  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "2       a116662e      dba9d8  The Midnight Library       Matt Haig    Fantasy          Online            Cash            21           18              30  Buy 3 Books, Get 1 Free                 0    Completed                No\n",
            "3       2db9cc65      7cdf6c                  Dune   Frank Herbert     Sci-Fi          Online            Cash            26           24              20        Flat 50% Discount                50    Completed                No\n",
            "4       d99cc2d4      eb00af                  Dune   Frank Herbert     Sci-Fi          Online            Card             6            5              10    Buy One, Get One Free                 0    Completed                No\n",
            "\n",
            "Column names in the combined dataset:\n",
            "['Transaction ID', 'Customer ID', 'Book Title', 'Author', 'Genre', 'Purchase Method', 'Payment Method', 'Stock Before', 'Stock After', 'Loyalty Points', 'Promotion Applied', 'Discount Applied', 'Order Status', 'Restock Triggered']\n",
            "\n",
            "Step 3: Verifying data integrity after combination...\n",
            "\n",
            "Missing values in each column of the combined dataset:\n",
            "Transaction ID          0\n",
            "Customer ID             0\n",
            "Book Title              0\n",
            "Author                  0\n",
            "Genre                   0\n",
            "Purchase Method         0\n",
            "Payment Method          0\n",
            "Stock Before            0\n",
            "Stock After             0\n",
            "Loyalty Points          0\n",
            "Promotion Applied    1575\n",
            "Discount Applied        0\n",
            "Order Status            0\n",
            "Restock Triggered       0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values in each column:\n",
            "Transaction ID       0.00\n",
            "Customer ID          0.00\n",
            "Book Title           0.00\n",
            "Author               0.00\n",
            "Genre                0.00\n",
            "Purchase Method      0.00\n",
            "Payment Method       0.00\n",
            "Stock Before         0.00\n",
            "Stock After          0.00\n",
            "Loyalty Points       0.00\n",
            "Promotion Applied   45.00\n",
            "Discount Applied     0.00\n",
            "Order Status         0.00\n",
            "Restock Triggered    0.00\n",
            "dtype: float64\n",
            "\n",
            "Number of duplicate rows: 0\n",
            "\n",
            "Statistical summary of numeric columns:\n",
            "       Stock Before  Stock After  Loyalty Points  Discount Applied\n",
            "count       3500.00      3500.00         3500.00           3500.00\n",
            "mean          27.76        25.77           13.63              6.61\n",
            "std           13.15        13.17           11.38             16.94\n",
            "min            5.00         2.00            0.00              0.00\n",
            "25%           16.00        14.00            0.00              0.00\n",
            "50%           28.00        26.00           10.00              0.00\n",
            "75%           39.00        37.00           20.00              0.00\n",
            "max           50.00        49.00           30.00             50.00\n",
            "\n",
            "Combined dataset saved as 'combined_bookstore_data.csv'\n",
            "\n",
            "Data import and combination process completed successfully.\n"
          ]
        }
      ]
    }
  ]
}